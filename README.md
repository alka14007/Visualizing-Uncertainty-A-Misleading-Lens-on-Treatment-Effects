# Visualizing-Uncertainty-A-Misleading-Lens-on-Treatment-Effects

Platform Used: Medium

Link to the article: https://medium.com/@alka14tomar/visualizing-uncertainty-a-misleading-lens-on-treatment-effects-c5f29fcb40de 

A sumaaru understanding of the paper: How Visualizing Inferential Uncertainty Can Mislead Readers About Treatment Effects in Scientific Results
(http://www.dangoldstein.com/papers/Hofman_Goldstein_Hullman_Visualizing_Uncertainty_Mislead_Scientific.pdf)

# Introduction

Scientists frequently express either inferential uncertainty (such as an error in the estimate of a population mean) or outcome uncertainty (such as variation of outcomes around that mean) about their estimates when presenting visualizations of experimental data. How does this decision affect readers’ perceptions of the magnitude of a treatment’s effects? The scientist has performed two experiments to understand the readers’ mindset and how they perceive the given visualizations comparing 95% confidence intervals (means and standard errors) to 95% prediction intervals (means and standard deviations). In the first experiment, it is discovered that when confidence intervals are displayed in comparison to prediction intervals, participants are more inclined to pay more for and overestimate the impact of a treatment. In the second experiment, different effect sizes are compared between alternative and conventional visualizations. We discover that axis rescaling decreases error, but not as well as prediction intervals or animated hypothetical outcome plots (HOPs), and that participants underestimate diversity in individual outcomes when inferential uncertainty is depicted.

An example to understand better the technical terms Inferential Uncertainty and Outcome Uncertainty:

For instance, inferential uncertainty can be used to describe how well one has calculated the average height of men and women based on the collection of measurements that were taken. A sizable sample allows for the precise estimation of each group’s average height. The fact that there is still a lot of outcome uncertainty, however, it is because individual heights within each group vary greatly from their respective averages.

Experiment 1

The goal of the first experiment was to determine how differently people interpret the same effect when it is presented using either of two communication techniques: (a) inferential uncertainty (population mean) or (b) outcome uncertainty (variation of outcomes around that mean).

Over 2400 participants participated in the investigation.

It was explained to the participants that they were athletes competing in a boulder sliding game against Blorg, another equally competent competitor. To slide a standard boulder on ice farther than your opponent is the object of the game. To increase their sliding distance for their next and final competition, participants had the option to hire a superior boulder (i.e., receive a treatment).

Result 1

Participants were presented with a visualization with data and text information about the standard and superior boulders. It was observed that when the visualization highlighted inferential uncertainty, individuals were willing to pay much more for the treatment (pay for superior boulder) overall.

![image](https://github.com/user-attachments/assets/97d76c05-c81d-455f-af64-ef7e25e738f4)

It follows that readers should focus more on experiments with higher means, even though the average could change or differ with just one positive or negative case of an individual.

Because scientific visualizations can mislead, it is better to present inferential uncertainty and outcome uncertainty side by side with additional captions and text so that readers may make an informed decision.


![image](https://github.com/user-attachments/assets/c4c0fad6-eede-459a-80b6-ea0a1af0e629)

Experiment 2

At this point, we all understand how crucial it is to create an accurate and effective visualization to convey the correct information to audiences.

From experiment 1 we understand that showing data with inferential uncertainty or with outcome uncertainty has its own shortcomings. In Experiment 2, scientists have tried to explore 2 alternative visualizations (other than inferential and outcome uncertinity) to make the understanding of scientific results better.

Result 2

![image](https://github.com/user-attachments/assets/50c864c6-5485-4011-bf8c-6590d7eb2e38)

Figure 2 (a) illustrates the first alternate visualization, where error bars represent 95% CIs, and the vertical axis has been adjusted to include 95% PIs. Similar to intentionally extending the y-axis range to include zero, which visualization experts sometimes suggest, this relatively straightforward modification has the advantage of communicating information about both inferential uncertainty through the error bars and outcome uncertainty through the axis range.

![image](https://github.com/user-attachments/assets/86fb3e0f-9bb2-4f30-8d9e-14533e0da09f)

The second alternative depiction is HOP (Hypothetical Outcome Plot), which uses animated frames to display samples from underlying distributions to illustrate uncertainty. The percentage of frames that preferentially sample from one distribution over another. They are tailored for the job and offer readers a helpful point of comparison.

# Conclusion

According to the mentioned study, it can be concluded that to effectively convey scientific findings to laypersons or laymen, it is essential to use simple language to explain the visualization and technical terms rather than alone scientific language and graphs that are difficult for the average person to understand. Involving the specifics of an individual outcomes (animated samples) is just as crucial as discussing the mean in order for a person to make an informed decision. Scientific results should also not only be based on the average of the experiments performed but also on the mean.

For the full paper:

How Visualizing Inferential Uncertainty Can Mislead Readers About Treatment Effects in Scientific Results

http://www.dangoldstein.com/papers/Hofman_Goldstein_Hullman_Visualizing_Uncertainty_Mislead_Scientific.pdf

Questions to Consider

1. A layman might not be able to decode the data filled with technicalities (e.g., words like standard errors, mean, standard deviation), do you agree with this premise? If so, does it make reading articles like these difficult?
Does adding meaningful explanation to the graphs along with the individual outcomes and population mean helps in understanding the results better, as seen in experiment #2?
